{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPh+K7maj6+yTAsvqk4rPiI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filipecancio/inteligencia-artifical-projetos/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Invocando o nltk:"
      ],
      "metadata": {
        "id": "A_ODGxw-372L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import floresta\n",
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "from nltk import word_tokenize, corpus\n",
        "\n",
        "nltk.download(\"all\")\n",
        "\n",
        "LINGUAGEM = \"portuguese\"\n",
        "\n",
        "def get_tokens(texto):\n",
        "    tokens = word_tokenize(texto, LINGUAGEM)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "AFJJVuc_4A6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar funções pra classificar gramaticalmente as palavras:"
      ],
      "metadata": {
        "id": "PJE6FMzM4FSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def grammar_types():\n",
        "  types = {}\n",
        "  for (key,type) in floresta.tagged_words():\n",
        "    if \"+\" in type:\n",
        "      type = type[type.index(\"+\")+1:]\n",
        "    types[key.lower()] = type\n",
        "  return types\n",
        "\n",
        "\n",
        "def get_type(tokens):\n",
        "  aaaa = {}\n",
        "  types = grammar_types()\n",
        "  for token in tokens:\n",
        "    aaaa = aaaa[token]\n",
        "    print(f\"a palavra {token} é um(a) {aaaa}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    phrase = \"a verdadeira generosidade para com o futuro consiste em dar tudo ao presente\"\n",
        "    tokens = get_tokens(phrase)\n",
        "    get_type(tokens)"
      ],
      "metadata": {
        "id": "PpHI4wYP4v2E",
        "outputId": "08c430a2-b39a-45e0-c8a4-e7f4219a0a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-4f2ae848b88e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mphrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"a verdadeira generosidade para com o futuro consiste em dar tudo ao presente\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-4f2ae848b88e>\u001b[0m in \u001b[0;36mget_type\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrammar_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0maaaa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maaaa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"a palavra {token} é um(a) {aaaa}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'a'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criando o stemizador:"
      ],
      "metadata": {
        "id": "Bt3GRf1i8g_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stem(tokens):\n",
        "  stemmer = RSLPStemmer()\n",
        "\n",
        "  for token in tokens:\n",
        "    print(f\"a raiz da palavra {token} é {stemmer.stem(token)}\")\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "  phrase = \"a verdadeira generosidade para com o futuro consiste em dar tudo ao presente\"\n",
        "  tokens = get_tokens(phrase)\n",
        "  get_stem(tokens)\n"
      ],
      "metadata": {
        "id": "knXHt1E98ANe",
        "outputId": "cd498743-d688-43d2-e98a-03591cbabcde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a raiz da palavra a é a\n",
            "a raiz da palavra verdadeira é verd\n",
            "a raiz da palavra generosidade é generos\n",
            "a raiz da palavra para é par\n",
            "a raiz da palavra com é com\n",
            "a raiz da palavra o é o\n",
            "a raiz da palavra futuro é futur\n",
            "a raiz da palavra consiste é cons\n",
            "a raiz da palavra em é em\n",
            "a raiz da palavra dar é dar\n",
            "a raiz da palavra tudo é tud\n",
            "a raiz da palavra ao é ao\n",
            "a raiz da palavra presente é pres\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caso queira fazer a lematização, usar a biblioteca SPACY"
      ],
      "metadata": {
        "id": "SAL3hrcp-GM_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__2yq_W10SEf"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "from nltk import word_tokenize, corpus\n",
        "from nltk.corpus import floresta\n",
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "nltk.download(\"all\")\n",
        "\n",
        "LINGUAGEM = \"portuguese\"\n",
        "\n",
        "def iniciar():\n",
        "    global classificacoes\n",
        "    global palavras_de_parada\n",
        "\n",
        "    palavras_de_parada = set(corpus.stopwords.words(LINGUAGEM))\n",
        "\n",
        "    classificacoes = []\n",
        "    for (palavra, classificacao) in floresta.tagged_words():\n",
        "        if \"+\" in classificacao:\n",
        "            classificacao = classificacao[classificacao.index(\"+\") + 1:]\n",
        "        \n",
        "        classificacoes.append((palavra.lower(), classificacao))\n",
        "\n",
        "\n",
        "def obter_tokens(texto):\n",
        "    tokens = word_tokenize(texto, LINGUAGEM)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def imprimir_tokens(tokens):\n",
        "    for token in tokens:\n",
        "        print(f\"token: {token}\")\n",
        "\n",
        "def eliminar_palavras_de_parada(tokens):\n",
        "    global palavras_de_parada\n",
        "\n",
        "    numero_de_palavras_eliminadas, tokens_filtrados = 0, []\n",
        "    for token in tokens:\n",
        "        if token not in palavras_de_parada:\n",
        "            tokens_filtrados.append(token)\n",
        "        else:\n",
        "            numero_de_palavras_eliminadas +=1\n",
        "\n",
        "    return numero_de_palavras_eliminadas, tokens_filtrados\n",
        "    \n",
        "def classificar_gramaticamente(tokens):\n",
        "    global classificacoes\n",
        "    \n",
        "    for token in tokens:\n",
        "        for (palavra, classificacao) in classificacoes:\n",
        "            if token == palavra:\n",
        "                print(\"token '\" + token + \"' = \", classificacao)\n",
        "                \n",
        "                break\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iniciar()\n",
        "\n",
        "    texto = \"a verdadeira generosidade para com o futuro consiste em dar tudo ao presente\"\n",
        "    tokens = obter_tokens(texto)\n",
        "    imprimir_tokens(tokens)\n",
        "\n",
        "    print(\"eliminando palavras de parada...\")\n",
        "    numero_de_palavras_eliminadas, tokens = eliminar_palavras_de_parada(tokens)\n",
        "    if numero_de_palavras_eliminadas:\n",
        "        print(f\"foram eliminadas {numero_de_palavras_eliminadas} palavra(s)\")\n",
        "        imprimir_tokens(tokens)\n",
        "        \n",
        "    print(\"classificando gramaticamente...\")\n",
        "    classificar_gramaticamente(tokens)\n",
        "    \n",
        "    print(\"estematizando...\")\n",
        "    estematizar(tokens)"
      ]
    }
  ]
}